# -*- coding: utf-8 -*-
"""dqn_test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aA5AYkealDJ7EaTb-GeBuLPRlcDhWS9l
"""

from google.colab import drive 
drive.mount('/content/drive')

from baselines.common.atari_wrappers import make_atari, wrap_deepmind
import numpy as np 
import tensorflow as tf 
from tensorflow import keras 
import gym 


def testing(model, env, n_episodes, rewards):
  seed = 42 
  
  model = tf.keras.models.load_model('/content/drive/MyDrive/game_ai/model/saved_model.pb')
  
  env = make_atari("BreakoutNoFrameskip-v4")
  env = wrap_deepmind(env, frame_stack=True, scale=True)
  env.seed(seed)
  
  env = gym.wrappers.Monitor(env,'/content/drive/MyDrive/game_ai/videos', video_callable=lambda episode_id: True, force=True)
  
  n_episodes = 10
  
  rewards = np.zeros(n_episodes, dtype=float)

  for i in range(n_episodes):
        state = np.array(env.reset())
        done = False

        while not done:
            # the action uses greedy policy 
            state_tensor = tf.convert_to_tensor(state)
            state_tensor = tf.expand_dims(state_tensor, 0)
            action_values = model.predict(state_tensor)
            action = np.argmax(action_values)
            # to find the action, reward and next state
            state_next, reward, done, _ = env.step(action)
            state = np.array(state_next)
            # rewards is updated
            rewards[i] += reward

 
  return rewards
  env.close()
print('Rewards: {}'. format(rewards))

import matplotlib.pyplot as plt

plot_rewards(episode_reward_history)
moving_average_rewards = np.convolve(episode_reward_history, np.ones(100) / 100, mode='valid')

plt.figure(figsize=(16, 10))
plt.plot(moving_average_rewards)
plt.xlabel('Episodes')
plt.ylabel('Rewards')
plt.title('Moving Average of Rewards')
plt.show()